# -*- coding: utf-8 -*-
"""Mini_LLM_from_Scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/170Xk_igYkWF7HZa8zw09ptsGow0pDiVf
"""

import torch
import torch.nn as nn
import math
import re
import random

torch.manual_seed(42)

text = """
artificial intelligence supports data analysis <eos>
artificial intelligence mimics human intelligence <eos>
artificial intelligence contributes to technological advancement <eos>
machine learning improves prediction accuracy <eos>
machine learning helps detect diseases early <eos>
ai tools support academic research <eos>
ai tools assist in problem solving <eos>
deep learning enhances pattern recognition <eos>
deep learning enables real time data processing <eos>
students use ai tools for learning <eos>
"""

class WordTokenizer:
    def __init__(self, text):
        words = re.findall(r"<eos>|\b\w+\b", text.lower())
        self.vocab = sorted(set(words))
        self.stoi = {w: i for i, w in enumerate(self.vocab)}
        self.itos = {i: w for i, w in enumerate(self.vocab)}

    def encode(self, text):
        return [self.stoi[w] for w in re.findall(r"<eos>|\b\w+\b", text.lower())]

    def decode(self, tokens):
        out = []
        for t in tokens:
            if self.itos[t] == "<eos>":
                break
            out.append(self.itos[t])
        return " ".join(out)

tokenizer = WordTokenizer(text)
data = torch.tensor(tokenizer.encode(text), dtype=torch.long)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.pe = pe.unsqueeze(0)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class TransformerBlock(nn.Module):
    def __init__(self, d):
        super().__init__()
        self.attn = nn.MultiheadAttention(d, 2, batch_first=True)
        self.ff = nn.Sequential(
            nn.Linear(d, 4*d),
            nn.ReLU(),
            nn.Linear(4*d, d)
        )
        self.ln1 = nn.LayerNorm(d)
        self.ln2 = nn.LayerNorm(d)

    def forward(self, x):
        a,_ = self.attn(x, x, x)
        x = self.ln1(x + a)
        x = self.ln2(x + self.ff(x))
        return x

class MiniLLM(nn.Module):
    def __init__(self, vocab_size, d=64):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d)
        self.pos = PositionalEncoding(d)
        self.block = TransformerBlock(d)
        self.fc = nn.Linear(d, vocab_size)

    def forward(self, x):
        x = self.pos(self.emb(x))
        x = self.block(x)
        return self.fc(x)

model = MiniLLM(len(tokenizer.vocab))

optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

seq_len = 6
epochs = 300

for _ in range(epochs):
    i = random.randint(0, len(data)-seq_len-1)
    x = data[i:i+seq_len].unsqueeze(0)
    y = data[i+1:i+seq_len+1].unsqueeze(0)
    loss = loss_fn(model(x).view(-1, len(tokenizer.vocab)), y.view(-1))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

def generate_text(prompt, attempts=5):
    best = ""
    for _ in range(attempts):
        tokens = tokenizer.encode(prompt)
        for _ in range(8):
            x = torch.tensor(tokens[-seq_len:]).unsqueeze(0)
            logits = model(x)[0,-1]
            logits[tokenizer.stoi["<eos>"]] *= 0.7
            next_token = torch.argmax(logits).item()
            if tokenizer.itos[next_token] == "<eos>":
                break
            tokens.append(next_token)
        sentence = tokenizer.decode(tokens)
        if 4 <= len(sentence.split()) <= 9:
            best = sentence
            break
    return best if best else sentence

print("GENERATED OUTPUT:\n")
for p in [
    "artificial intelligence",
    "machine learning",
    "ai tools",
    "deep learning",
    "students use ai"
]:
    print("-", generate_text(p))





